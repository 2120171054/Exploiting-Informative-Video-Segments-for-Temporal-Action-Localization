This repo holds the codes of STAN+PGCN in paper: "Exploiting Informative Video Segments for Temporal Action Localization".



## Update
30/11/2020 We updated the Informal Version of code.



## Usage Guide
#Configure Environments
pip install -r requirements.txt

#Data Preparation
Download I3D features and record their "p_train" and "p_test"
Download proposal lists, and put them in ./data/

#Training
Use "p_train" and "p_test" to set "train_ft_path" and "test_ft_path" in ./data/dataset_cfg.yaml
python pgcn_train.py thumos14 --snapshot_pre ./save_model/

#Testing
sh test.sh ./save_model/YOUR_RECORD_MODEL
After two-stream training and testing
sh test_two_stream.sh

| mAP@0.5IoU (%)                    | RGB   | Flow  | RGB+Flow      |
|-----------------------------------|-------|-------|---------------|
| P-GCN (I3D)                       | 40.89 | 49.87 | 52.65 |



## Reference
My implementations borrow ideas from: 
BSN: Boundary Sensitive Network for Temporal Action Proposal Generation.
Graph Convolutional Networks for Temporal Action Localization

## Contact
sunche@bit.edu.cn
